{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3084a00",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 36px;\">SAR tutorial</h1>\n",
    "\n",
    "In this notebook, we introduce synergistic action representations (SAR) and demonstrate how to use these representations to facilitate the training of high-dimensional continuous control policies within MyoSuite.\n",
    "\n",
    "The primary purpose of this notebook is to equip the user to train with SAR for their own task specification(s) both within and beyond the musculoskeletal control paradigm.\n",
    "\n",
    "\n",
    "# Table of contents\n",
    "\n",
    "- [Imports and utilities](#imports-and-utilities)\n",
    "- [Walkthrough of SAR core functions](#SAR-core-functions)\n",
    "- [Example 1: SAR x physiological locomotion](#TODO)\n",
    "- [Example 2: SAR x physiological dexterity](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5eddd",
   "metadata": {},
   "source": [
    "# Imports and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe8e7",
   "metadata": {},
   "source": [
    "First, we import the packages that SAR requires, including OpenAI gym, MyoSuite, stable-baselines3, and sci-kit learn. It is recommended to `pip install` any imports that are missing in your environment.\n",
    "\n",
    "We also import additional packages for video rendering, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f610",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3==1.7.0\n",
    "# !pip install joblib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60114185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for SAR\n",
    "from SAR_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env_name, episodes=10, timesteps=50):\n",
    "    \"\"\"\n",
    "    Displays any mp4 video within the notebook.\n",
    "    \n",
    "    video_path: str; path to mp4\n",
    "    video_width: str; optional; size to render video\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    # get rid of skin         \n",
    "    geom_1_indices = np.where(env.sim.model.geom_group == 1)\n",
    "    env.sim.model.geom_rgba[geom_1_indices, 3] = 0\n",
    "    frames = []\n",
    "    for __ in tqdm(range(episodes)):\n",
    "        env.reset()\n",
    "        # get muscle activations         \n",
    "        for _ in range(timesteps):\n",
    "            frame = env.sim.renderer.render_offscreen(width=640, height=480,camera_id='side_view')\n",
    "            frames.append(frame)\n",
    "            act = env.action_space.sample()\n",
    "            a,b,c,d= env.step(act) # take a random action\n",
    "    env.close()\n",
    "\n",
    "    # make a local copy\n",
    "    skvideo.io.vwrite(f'test_env_temp.mp4', np.asarray(frames),outputdict={\"-pix_fmt\": \"yuv420p\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fff195",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_env('myoLegStairTerrainWalk-v0', episodes=1, timesteps=25)\n",
    "show_video('test_env_temp.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(env_name, model_path, env_path, episodes, video_name, determ=False, \n",
    "            pca=None, ica=None, scaler=None, phi=None, is_sar=False, syn_nosyn=False):\n",
    "    frames = []\n",
    "    if is_sar:\n",
    "        if syn_nosyn:\n",
    "            env = SynNoSynWrapper(gym.make(env_name), ica, pca, scaler, phi)\n",
    "        else:\n",
    "            env = SynergyWrapper(gym.make(env_name), ica, pca, scaler)\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "\n",
    "    camera = 'side_view'\n",
    "        \n",
    "    for i,__ in tqdm(enumerate(range(episodes))):              \n",
    "        env.reset()\n",
    "\n",
    "        model = SAC.load(model_path)\n",
    "        vec = VecNormalize.load(env_path, DummyVecEnv([lambda: env]))\n",
    "        \n",
    "        rs = 0\n",
    "        is_solved = []\n",
    "        done = False\n",
    "        for _ in range(1000):\n",
    "            o = vec.normalize_obs(env.get_obs())\n",
    "            a, __ = model.predict(o, deterministic=determ)\n",
    "            \n",
    "            frame = env.sim.renderer.render_offscreen(width=640, height=480,camera_id=camera)\n",
    "            frames.append(frame)\n",
    "\n",
    "            next_o, r, done, info = env.step(a)\n",
    "            is_solved.append(info['solved'])\n",
    "            \n",
    "            rs+=r\n",
    "    env.close()\n",
    "    skvideo.io.vwrite(f'{video_name}.mp4', np.asarray(frames),outputdict={\"-pix_fmt\": \"yuv420p\"})\n",
    "    \n",
    "ica,pca,norm = load_locomotion_SAR()\n",
    "env_path = 'multihill_terrain_syn_env_myoLegWalkHillyTerrain-v0_3'\n",
    "model_path = 'multihill_terrain_syn_model_myoLegWalkHillyTerrain-v0_3.zip'\n",
    "get_video('Test-v0',model_path,env_path,1,'test',pca=pca,ica=ica,scaler=norm,is_sar=True)\n",
    "show_video('test.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc299ac0",
   "metadata": {},
   "source": [
    "# SAR core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8433315",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "         Next, we'll define and implement the core functions for SAR. The framework follows the basic structure displayed on the right. Accordingly, we will define functions that do all of the following:\n",
    "        <br>\n",
    "        <br>\n",
    "        <ol>\n",
    "            <li>Train a policy on a simpler task</li>\n",
    "            <li>Rollout and capture muscle activation data from this trained policy</li>\n",
    "            <li>Compute SAR from this activation data</li>\n",
    "            <li>Use SAR to train on a harder task (see policy architecture below)</li>\n",
    "        </ol>\n",
    "        <br>\n",
    "        <br>\n",
    "        We can now define functions to implement each of these steps.\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"./SAR_images/framework.png\" alt=\"Framework\" style=\"width:200px;\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125c47",
   "metadata": {},
   "source": [
    "### Train a policy on a simpler task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24053a43",
   "metadata": {},
   "source": [
    "`train()` enables the simple training of a policy using the stable-baselines3 implementation of SAC on a desired environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, policy_name, timesteps, seed):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=1000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30fc82",
   "metadata": {},
   "source": [
    "### Extracting muscle activation data from a trained policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350c00b",
   "metadata": {},
   "source": [
    "`get_activations()` extracts muscle activation data at each timestep of the trained policy's rollouts. In this implementation, `get_activations()` saves muscle activations from a given rollout if one of two conditions is met: (a) the task is solved during that rollout, or (b) the rewards from that rollout fall above a certain threshold, computed as a percentile from a set of preview rollouts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be60e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(name, env_name, seed, episodes=1000, percentile=80):\n",
    "    \"\"\"\n",
    "    Returns muscle activation data from N runs of a trained policy.\n",
    "\n",
    "    name: str; policy name (see train())\n",
    "    env_name: str; name of the gym environment\n",
    "    seed: str; seed of the trained policy\n",
    "    episodes: int; optional; how many rollouts?\n",
    "    mode: str; optional; 'solved' or 'pos_reward'\n",
    "    percentile: int; optional; percentile to set the reward threshold for considering an episode as successful\n",
    "    \"\"\"\n",
    "    with gym.make(env_name) as env:\n",
    "        env.reset()\n",
    "\n",
    "        model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "        vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "\n",
    "        # Calculate the reward threshold from 100 preview episodes\n",
    "        preview_rewards = []\n",
    "        for _ in range(100):\n",
    "            env.reset()\n",
    "            rewards = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                rewards += r\n",
    "            preview_rewards.append(rewards)\n",
    "        reward_threshold = np.percentile(preview_rewards, percentile)\n",
    "\n",
    "        # Run the main episode loop\n",
    "        solved_eps = []\n",
    "        solved_acts = []\n",
    "        for i, _ in tqdm(enumerate(range(episodes))):\n",
    "            env.reset()\n",
    "            rewards, is_solved, acts = 0, [], []\n",
    "            done = False\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                is_solved.append(info['solved'])\n",
    "                acts.append(env.sim.data.act.copy())\n",
    "\n",
    "                rewards += r\n",
    "            if 'Leg' in env_name:\n",
    "                if rewards > reward_threshold:\n",
    "                    solved_eps.append(1)\n",
    "                    solved_acts.extend(acts)\n",
    "                else:\n",
    "                    solved_eps.append(0)\n",
    "            else:\n",
    "                if sum(is_solved) > 0 or rewards > reward_threshold:\n",
    "                    solved_eps.append(1)\n",
    "                    solved_acts.extend(acts)\n",
    "                else:\n",
    "                    solved_eps.append(0)\n",
    "\n",
    "        print(f'{np.mean(solved_eps)*100}% solved over {episodes} rollouts.')\n",
    "    return solved_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc01ec",
   "metadata": {},
   "source": [
    "### Computing SAR using muscle activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3cdc",
   "metadata": {},
   "source": [
    "`find_synergies` returns a dictionary (and optionally, a plot) that shows variance accounted for (VAF) by N synergies  from the original muscle activation data. In general, we find that using a number of synergies where VAF > 80% leads to good performance, though this heuristic may not hold true in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synergies(acts, plot=True):\n",
    "    \"\"\"\n",
    "    Computed % variance explained in the original muscle activation data with N synergies.\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    plot: bool; whether to plot the result\n",
    "    \"\"\"\n",
    "    syn_dict = {}\n",
    "    for i in range(39):\n",
    "        pca = PCA(n_components=i+1)\n",
    "        _ = pca.fit_transform(acts)\n",
    "        syn_dict[i+1] =  round(sum(pca.explained_variance_ratio_), 4)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(list(syn_dict.keys()), list(syn_dict.values()))\n",
    "        plt.title('VAF by N synergies')\n",
    "        plt.xlabel('# synergies')\n",
    "        plt.ylabel('VAF')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    return syn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13361813",
   "metadata": {},
   "source": [
    "Once the number of synergies to use is determined (above), `compute_SAR()` will take the activation data as input and yield the synergistic action representation (SAR) in the form of ICA, PCA, and normalizer objects. We use a linear representation to approximate SAR given its validation in the motor neuroscience literature (e.g., [Tresch et al, 2006](https://journals.physiology.org/doi/epdf/10.1152/jn.00222.2005)). Such representations are also highly interpretable and easy to implement with `sci-kit learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SAR(acts, n_syn, save=False):\n",
    "    \"\"\"\n",
    "    Takes activation data and desired n_comp as input and returns/optionally saves the ICA, PCA, and Scaler objects\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    n_comp: int; number of synergies to use\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_syn)\n",
    "    pca_act = pca.fit_transform(acts)\n",
    "    \n",
    "    ica = FastICA()\n",
    "    pcaica_act = ica.fit_transform(pca_act)\n",
    "    \n",
    "    normalizer = MinMaxScaler((-1, 1))    \n",
    "    normalizer.fit(pcaica_act)\n",
    "    \n",
    "    if save:\n",
    "        joblib.dump(ica, 'ica_demo.pkl') \n",
    "        joblib.dump(pca, 'pca_demo.pkl')\n",
    "        joblib.dump(normalizer, 'scaler_demo.pkl')\n",
    "    \n",
    "    return ica, pca, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60f8d5",
   "metadata": {},
   "source": [
    "### Train with SAR on a more challenging task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c987cc",
   "metadata": {},
   "source": [
    "In order to train with SAR, we initialize a `SynNoSynWrapper`, which implements the policy architecture presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d499c17",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/pol_arch.png\" alt=\"Policy Architecture\" style=\"width:700px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynNoSynWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the combination of a task-general synergy space and a\n",
    "    task-specific orginal space, and uses this mix to step the environment in the original action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, scaler, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = scaler\n",
    "        self.weight = phi\n",
    "        \n",
    "        self.syn_act_space = self.pca.components_.shape[0]\n",
    "        self.no_syn_act_space = env.action_space.shape[0]\n",
    "        self.full_act_space = self.syn_act_space + self.no_syn_act_space\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.full_act_space,),dtype=np.float32)\n",
    "    def action(self, act):\n",
    "        syn_action = act[:self.syn_act_space]\n",
    "        no_syn_action = act[self.syn_act_space:]\n",
    "        \n",
    "        syn_action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([syn_action])))[0]\n",
    "        final_action = self.weight * syn_action + (1 - self.weight) * no_syn_action\n",
    "        \n",
    "        return final_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d80be",
   "metadata": {},
   "source": [
    "Additionally, we initialize a `SynergyWrapper`, which implements a variation of the above policy architecture that only leverages task-general synergistic activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynergyWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the synergy space and inverse transforms\n",
    "    synergy-exploiting actions back into the original muscle activation space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, w_scaler):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = w_scaler\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.pca.components_.shape[0],),dtype=np.float32)\n",
    "    \n",
    "    def action(self, act):\n",
    "        action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([act])))\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070abb8",
   "metadata": {},
   "source": [
    "We then train a policy with these wrappers using `SAR_RL`. We find that a blend weight $\\varphi$=0.66 between the synergistic (task-general) and nonsynergistic (task-specific) activations works best in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAR_RL(env_name, policy_name, timesteps, seed, ica, pca, normalizer, phi=.66, syn_nosyn=True):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC + SynNoSynWrapper.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    ica: the ICA object\n",
    "    pca: the PCA object\n",
    "    normalizer: the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    \"\"\"\n",
    "    if syn_nosyn:\n",
    "        env = SynNoSynWrapper(gym.make(env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = SynergyWrapper(gym.make(env_name), ica, pca, normalizer)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=5000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d9302",
   "metadata": {},
   "source": [
    "# Full training example 1: SAR x physiological locomotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c4882",
   "metadata": {},
   "source": [
    "Now that we have defined the core functions for implementing SAR, we will now train a locomotion policy using MyoLegs. In this example:\n",
    "- a policy is trained on straight flat walking task for 1.5M steps (`myoLegWalk-v0`)\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- Variance accounted for (VAF) by N synergies is computed. Here, we use 20 synergies from the 80-dimensional leg muscle activations\n",
    "- SAR is used to train on:\n",
    "    - a hilly terrain walking task (`myoLegHillyTerrainWalk-v0`)\n",
    "    - an uneven terrain walking task (`myoLegRoughTerrainWalk-v0`)\n",
    "    - a stair climbing task (`myoLegStairTerrainWalk-v0`)\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc17b86",
   "metadata": {},
   "source": [
    "# Step 1.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `myoLegWalk-v0` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de557",
   "metadata": {},
   "source": [
    "## Option 1.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29fc68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train('myoLegWalk-v0', 'play_period', 1.5e6, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_play_period_video'\n",
    "get_vid(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "muscle_data = get_activations(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc6df4",
   "metadata": {},
   "source": [
    "## Option 1.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = load_locomotion_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e199df",
   "metadata": {},
   "source": [
    "# Step 1.2: Train on unseen terrain with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose terrain to learn, or manually choose one\n",
    "new_terrain = np.random.choice(['Hilly', 'Stairs', 'Rough'])\n",
    "# new_terrain = ...\n",
    "print(f'myoLeg{new_terrain}TerrainWalk-v0 selected for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101f52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAR_RL(env_name=f'myoLeg{new_terrain}TerrainWalk-v0', policy_name='SAR-RL', timesteps=2.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66, syn_nosyn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_SAR-RL_video'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name=f'myoLeg{new_terrain}TerrainWalk-v0', seed='0', episodes=5, video_name=video_name, \n",
    "        determ=False, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=False)\n",
    "\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10cb1e",
   "metadata": {},
   "source": [
    "# Step 1.3 (optional): Train on unseen terrain with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(f'myoLeg{new_terrain}TerrainWalk-v0', 'RL-E2E', 4e6, '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16629ae9",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac0f3",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9424151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='locomotion', terrain=new_terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf790b",
   "metadata": {},
   "source": [
    "# Full training example 2: SAR x physiological manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cbb1e",
   "metadata": {},
   "source": [
    "Now, we turn to using synergies for manipulation rather than locomotion. In this example:\n",
    "- a policy is trained on an eight-object reorientation task, `Reorient8-v0`.\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- Variance accounted for (VAF) by N synergies is computed. Here, we select N where VAF > 0.8.\n",
    "- SAR is computed at this VAF threshold.\n",
    "- SAR is used to train on a 100-object reorientation task, `Reorient100-v0`.\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1963d71",
   "metadata": {},
   "source": [
    "# Step 2.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `Reorient8` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c960e8",
   "metadata": {},
   "source": [
    "## Option 2.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a policy is trained on an eight-object reorientation task, Reorient8-v0\n",
    "train(env_name='myoHandReorient8-v0', policy_name='play_period', timesteps=1e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f34571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after training, we collect muscle activation data from policy rollouts\n",
    "muscle_data = get_activations(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a109e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAF by N synergies is computed. Here, N is selected where VAF > 0.8\n",
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'play_period_vid'\n",
    "get_vid(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR is computed at this VAF threshold\n",
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e6fd8",
   "metadata": {},
   "source": [
    "## Option 2.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592deab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO USE PRECOMPUTED SAR\n",
    "ica,pca,normalizer = load_manipulation_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be153658",
   "metadata": {},
   "source": [
    "# Step 2.2: Train on Reorient100 with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf7de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SAR is used to train on a 100-object reorientation task, Reorient100-v0\n",
    "SAR_RL(env_name='myoHandReorient100-v0', policy_name='SAR-RL', timesteps=1.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'SAR_vid'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name='myoHandReorient100-v0', seed='0', episodes=10, video_name=video_name, \n",
    "        determ=True, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=True)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc00d6",
   "metadata": {},
   "source": [
    "# Step 2.3 (optional): Train on Reorient100 with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d1",
   "metadata": {},
   "source": [
    "We can compare the performance of (A) pretraining for 1M steps on `Reorient8-v0`, getting SAR, and training with SAR-RL on `Reorient100-v0` for 2M steps with (B) training for 3M steps using end-to-end RL ('RL-E2E')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d36ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(env_name='myoHandReorient100-v0', policy_name='RL-E2E', timesteps=2.5e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'RL+E2E_vid'\n",
    "get_vid('RL-E2E', 'myoHandReorient100-v0', '0', determ=False, episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c777",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1717e5",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05215d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='manipulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bc976",
   "metadata": {},
   "source": [
    "# Step 2.5 (optional): zero-shot testing manipulation policies on new unseen objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bba92d",
   "metadata": {},
   "source": [
    "We can optionally test these manipulation policies on unseen objects to determine their generalizability. Here, we implement two test\n",
    "environments.\n",
    "\n",
    "1. ReorientID-v0—1000 unseen objects generated by sampling from the same dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with the same kind of shapes as those seen in the training set.\n",
    "\n",
    "2. ReorientOOD-v0—1000 unseen objects generated by sampling from different dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with different kinds of shapes as those seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba66889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_test(name, test_env_name, env_name='myoHandReorient100-v0', seed='0', determ=True, ica=None, \n",
    "                  pca=None, normalizer=None, phi=.66, episodes=500, is_sar=False, syn_nosyn=False):\n",
    "    \"\"\"\n",
    "    Check zero-shot performance of policies on the test environments.\n",
    "    \n",
    "    name: str; name of the policy to test\n",
    "    env_name: str; name of gym env the policy to test was trained on (Reorient100-v0).\n",
    "    seed: str; seed of the policy to test\n",
    "    test_env_name: str; name of the desired test env\n",
    "    ica: if testing SAR-RL, the ICA object\n",
    "    pca: if testing SAR-RL, the PCA object\n",
    "    normalizer: if testing SAR-RL, the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    episodes: int; number of episodes to run on the test environment\n",
    "    \"\"\"\n",
    "    if is_sar:\n",
    "        if syn_nosyn:\n",
    "            env = SynNoSynWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "        else:\n",
    "            env = SynergyWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = gym.make(test_env_name)\n",
    "    env.reset()\n",
    "\n",
    "    model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "    vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "    solved = []\n",
    "    for i,_ in enumerate(range(episodes)):\n",
    "        is_solved = []\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            o = env.get_obs()\n",
    "            o = vec.normalize_obs(o)\n",
    "            a, __ = model.predict(o, deterministic=determ)\n",
    "            next_o, r, done, info = env.step(a)\n",
    "            is_solved.append(info['solved'])\n",
    "        \n",
    "        if sum(is_solved) > 0:\n",
    "            solved.append(1)\n",
    "        else:\n",
    "            solved.append(0)\n",
    "\n",
    "    env.close()\n",
    "    return np.mean(solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08578ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing RL-E2E policy\n",
    "\n",
    "name = 'RL-E2E'\n",
    "\n",
    "e2e_id = zeroshot_test(name, 'myoHandReorientID-v0')\n",
    "\n",
    "e2e_ood = zeroshot_test(name, 'myoHandReorientOOD-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c97370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing SAR-RL policy\n",
    "\n",
    "name = 'SAR-RL'\n",
    "\n",
    "sar_id = zeroshot_test(name, 'myoHandReorientID-v0', ica=ica, pca=pca, \n",
    "                       normalizer=normalizer, is_sar=True, syn_nosyn=True)\n",
    "\n",
    "sar_ood = zeroshot_test(name, 'myoHandReorientOOD-v0', ica=ica, pca=pca, \n",
    "                        normalizer=normalizer, is_sar=True, syn_nosyn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cfe1b",
   "metadata": {},
   "source": [
    "We can visualize these zero-shot generalization results by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18035c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "    \n",
    "zeroshot = {\"SAR-RL\": [sar_id,sar_ood],\n",
    "            \"RL-E2E\": [e2e_id,e2e_ood]}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_zeroshot(ax, zeroshot)\n",
    "fig.set_size_inches(6, 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
